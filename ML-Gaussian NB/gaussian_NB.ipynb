{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model using the gaussian NB model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Paths:  \n",
    "- training data: data/sonar_train.data\n",
    "- validation data: data/sonar_valid.data\n",
    "- test data: data/sonar_test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 69.23076923076923 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#read file and store data to X and Y \n",
    "def read_data(filename): \n",
    "    file = open(filename, 'r', encoding='utf-8-sig')\n",
    "    dataset = []\n",
    "    for line in file:\n",
    "        data = line.split(',')\n",
    "        x_data = [float(x) for x in data[:-1]] \n",
    "        y_data = int(data[len(data)-1])\n",
    "        dataset.append((x_data, y_data))\n",
    "\n",
    "    X = np.array([x for x, y in dataset])\n",
    "    Y = np.array([y for x, y in dataset])\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "#compute mean for each feature\n",
    "def compute_mean(partition_set): \n",
    "    means = np.mean(partition_set, axis=0)\n",
    "    return means\n",
    "\n",
    "#compute sd for each feature\n",
    "def compute_sd(partition_set): \n",
    "    standard_devs = np.std(partition_set, axis=0)\n",
    "    return standard_devs\n",
    "\n",
    "#calculate mean and sd for each features in each class\n",
    "def train(X, Y): \n",
    "    num_data, num_feature = X.shape\n",
    "    classes, class_count = np.unique(Y, return_counts=True)\n",
    "    prob_classes = []\n",
    "    mean_matrix = []   #row = classes, col = features\n",
    "    sd_matrix = []\n",
    "\n",
    "    #get probabilty of each y in the data\n",
    "    for count in class_count: \n",
    "       prob = count / num_data\n",
    "       prob_classes.append(prob)\n",
    "\n",
    "    for y in classes: \n",
    "        #get subset of X where label = y\n",
    "        y_indices = np.where(Y == y)[0]\n",
    "        partition_set = X[y_indices]\n",
    "\n",
    "        mean_matrix.append(compute_mean(partition_set))\n",
    "        sd_matrix.append(compute_sd(partition_set))\n",
    "    \n",
    "    return classes, np.array(prob_classes), np.array(mean_matrix), np.array(sd_matrix)\n",
    "\n",
    "#compute log likelihood p(fi|yi)\n",
    "def compute_likelihood(x, mean, sd): \n",
    "    likelihood = (1 / (sd * np.sqrt(2*np.pi))) * np.exp((-(x-mean) ** 2) / (2 * (sd ** 2)))\n",
    "    return likelihood\n",
    "\n",
    "\n",
    "#predict y for new x\n",
    "def prediction(x, classes, prob_classes, mean_matrix, sd_matrix): \n",
    "    best_y = classes[0]\n",
    "    best_posterior = float('-inf')\n",
    "\n",
    "    #compute posterior for each y and find the largest one\n",
    "    for i in range(len(classes)): \n",
    "        #posterior = p(yi) * p(f1|yi) * p(f2|yi) * ...\n",
    "        posterior = prob_classes[i]\n",
    "        for f in range(mean_matrix.shape[1]): \n",
    "            posterior *= compute_likelihood(x[f], mean_matrix[i][f], sd_matrix[i][f])\n",
    "\n",
    "        #compare posterior \n",
    "        if posterior > best_posterior: \n",
    "            best_posterior = posterior\n",
    "            best_y = classes[i]\n",
    "\n",
    "    return best_y\n",
    "\n",
    "#training dataset\n",
    "train_X, train_Y = read_data('data/sonar_train.data')\n",
    "X, Y = read_data('data/sonar_test.data')\n",
    "\n",
    "#train model \n",
    "classes, prob_classes, mean_matrix, sd_matrix = train(train_X, train_Y)\n",
    "\n",
    "#prediction \n",
    "correct_predictions = 0\n",
    "total_data = X.shape[0]\n",
    "for x, y in zip(X, Y): \n",
    "    pred_y = prediction(x, classes, prob_classes, mean_matrix, sd_matrix)\n",
    "\n",
    "    if pred_y == y: \n",
    "        correct_predictions += 1\n",
    "\n",
    "#accuracy \n",
    "accuracy = (correct_predictions / total_data) * 100\n",
    "print(\"accuracy:\", accuracy, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
